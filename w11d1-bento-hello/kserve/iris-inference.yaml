# kserve/iris-inference.yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: iris-bento
  namespace: ml
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    annotations:
      autoscaling.knative.dev/metric: concurrency
      autoscaling.knative.dev/target: "5"
    minReplicas: 1
    maxReplicas: 3
    containers:
      - name: user-container
        image: 014498620948.dkr.ecr.us-east-1.amazonaws.com/w11d1_bento_hello:latest
        ports:
          - containerPort: 3000
        readinessProbe:
          httpGet:
            path: /healthz
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "1"
            memory: "1Gi"

